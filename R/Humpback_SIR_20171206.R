# R Script for Bayesian Assessment Model of Humpback Whales - uses the equations
# in Zerbini et al. (2011) Code cleaned up and sent to Andre Punt, John Best and
# Grant Adams on 7 Dec 2017

#' HUMPBACK SIR controls the sampling from the priors, the bisection and
#' likelihoods and the output functions
#'
#' @param file.name name of a file to identified the files exported by the
#'   function
#' @param n.resamples number of resamples to compute the marginal posterior
#'   distributions
#' @param priors List of priors, usually generated using \link{make_prior_list}.
#'   Default is the default of \code{make_prior_list}. See details.
#' @param target.Yr year of the target population estimate for the bisection
#'   method. Default is 2008
#' @param num.haplotypes number of haplotypes to compute minimum viable
#'   population (from Jackson et al., 2006 and IWC, 2007)
#' @param output.Yrs year for outputing the predicted abundance estimates.
#'   Default is 2008, but multiple years can be specified. For example, if
#'   outputs for 2005 and 2008 are needed, output.Yrs = c(2005, 2008)
#' @param abs.abundance R object containing year, estimate of absolute
#'   abundance, and CV (see example)
#' @param rel.abundance R object containing years, estimates of relative
#'   abudnance and CVs (see example)
#' @param rel.abundance.key key to speficy if relative abundance data are used
#'   in the likelihood. Default is TRUE
#' @param count.data R object containing years, estimates of counts and effort.
#'   NOT USED
#' @param count.data.key key to speficy in count data are used. Default is
#'   FALSE. NOT USED
#' @param growth.rate.obs observed growth rate (1st element) and standard error
#'   (2nd element) as in Zerbni et al. (2011). If third element is FALSE, the
#'   growth rate is not included in the likelihood
#' @param growth.rate.Yrs Years for which the growth.rate.obs were computed (as
#'   in Zerbini et al., 2011)
#' @param catch.data R object containing the years and catches (see example)
#' @param control A list of control parameters, usually generated by
#'   \code{sir_control}.
#'
#' @return A \code{list} containing posterior samples and metadata
#'
#' TODO: Add the negative binomial likelihood for the count data, which is not
#' currently used even though it is defined in the main function call.
#'
#' Current default prior specification:
#' \code{
#' make_prior_list(r_max = make_prior(runif, 0, 0.106),
#'                 K = make_prior(use = FALSE),
#'                 N_obs = make_prior(runif, 500, 20000),
#'                 add_CV = make_prior(use = FALSE),
#'                 z = make_prior(2.39),
#'                 q_IA = make_prior(use = FALSE),
#'                 q_count = make_prior(use = FALSE)}
#'
#' @export
#'
#' @examples
#'
#' \dontrun{
#' HUMPBACK.SIR(file.name = "test.N2005",
#'              n.resamples = 100,
#'              priors = make_prior_list(),
#'              Klim = c(1, 500000),
#'              target.Yr = 2005,
#'              num.haplotypes = 0,
#'              tolerance.for.bisection = 0.0001,
#'              output.Yrs = c(2005, 2006),
#'              abs.abundance = Abs.Abundance.2005,
#'              rel.abundance = Rel.Abundance,
#'              rel.abundance.key = TRUE,
#'              count.data = Count.Data,
#'              count.data.key = FALSE,
#'              growth.rate.obs = c(0.074, 0.033, TRUE),
#'              growth.rate.Yrs = c(1995, 1996, 1997, 1998),
#'              catch.data = Catch.data,
#'              control = sir_control())
HUMPBACK.SIR <- function(file.name = "NULL",
                         n.resamples = 1000,
                         priors = make_prior_list(),
                         target.Yr = 2008,
                         num.haplotypes = 66,
                         output.Yrs = c(2008),
                         abs.abundance = Abs.Abundance,
                         rel.abundance = Rel.Abundance,
                         rel.abundance.key = TRUE,
                         count.data = NULL,
                         count.data.key = FALSE,
                         growth.rate.obs = c(0.074, 0.033, TRUE),
                         growth.rate.Yrs = c(1995, 1996, 1997, 1998),
                         catch.data = Catch.data,
                         control = sir_control()) {
  begin.time <- Sys.time()

  ################################
  # Assigning variables
  ################################
  target.Yr <- target.Yr
  ## Use the first year of the projection is set as the first year in the
  ## catch series
  start_Yr <- catch.data$Year[1]
  ## The last year of the projection is set as the last year in the catch or
  ## abundance series, whichever is most recent
  end.Yr <- max(tail(catch.data$Year, 1),
                max(abs.abundance$Year),
                max(rel.abundance$Year))
  ## Setting the target year for the bisection method
  bisection.Yrs <- target.Yr-start_Yr + 1
  ## Setting the years to project
  projection.Yrs <- end.Yr-start_Yr + 1

  ## Assigning the catch data
  catches <- catch.data$Catch
  ## Determining the number of Indices of Abundance available
  num.IA <- max(rel.abundance$Index)
  ## Determining the number of Count Data sets available
  num.Count <- max(count.data$Index)
  ## Computing the value of sigma as in Zerbini et al. 2011
  rel.abundance$Sigma <- sqrt(log(1 + rel.abundance$CV.IA.obs^2))
  ## Computing the value of sigma for the count data as in Zerbini et al. (2011)
  count.data$Sigma <- sqrt(log(1 + count.data$CV.IA.obs^2))
  ## Computing the value of sigma as in Zerbini et al. 2011
  abs.abundance$Sigma <- sqrt(log(1 + abs.abundance$CV.obs^2))
  ## Computing the minimum viable population, if num.haplotypes=0, assumes no MVP
  MVP <- 4 * num.haplotypes

  ## Start the loop
  i <- 0
  ## Keep track of number of draws
  draw <- 1
  Cumulative.Likelihood <- 0

  #Creating output vectors
  #-------------------------------------
  names <- c("r_max", "K", "sample.N.obs", "add_CV", "Nmin", "YearMin",
             "violate_MVP", paste("N", output.Yrs, sep = ""),
             paste("ROI_IA", unique(rel.abundance$Index), sep = ""),
             paste("q_IA", unique(rel.abundance$Index), sep = ""),
             paste("ROI_Count", unique(count.data$Index), sep = ""),
             paste("q_Count", unique(count.data$Index), sep = ""),
             "NLL.IAs", "NLL.Count", "NLL.N", "NLL.GR", "NLL", "Likelihood",
             "Max_Dep", paste("status", output.Yrs, sep = ""), "draw", "save")


  samples.output <- matrix(0, nrow = 1, ncol = length(names))
  resamples.output <- matrix(0, nrow = 1, ncol = length(names))
  resamples.trajectories <- matrix(NA, nrow = 1, ncol = projection.Yrs)
  final.trajectory <- matrix(NA, nrow = projection.Yrs, ncol = 6)
  Year <- seq(start_Yr, end.Yr, by = 1)

  if (control$progress_bar) {
    pb <- txtProgressBar(min = 0, max = n.resamples, style = 3)
  }

  #Initiating the SIR loop
  while (i < n.resamples) {
    #Sampling from Priors
    #-------------------------------
    save <- FALSE #variable to indicate whether a specific draw is kept

    #Sampling for r_max
    sample.r_max <- 0.2 #setting sample.r_max outside of the bound
    ## FIXME Why is this check necessary; just set the bounds using the prior?
    while (sample.r_max < priors$r_max$pars[1] | sample.r_max > priors$r_max$pars[2]) {
      ## Prior on r_max, keep if within boundaries
      sample.r_max <- priors$r_max$rfn()
    }

    ## Sampling from the N.obs prior
    sample.N.obs <- priors$N_obs$rfn()

    ## Prior on additional CV
    if (priors$add_CV$use) {
      sample.add_CV <- priors$add_CV$rfn()
    } else {
      sample.add_CV <- 0
    }

    ## Sample from prior for `z` (usually constant)
    sample.z <- priors$z$rfn()

    ## Sampling from q priors if q.prior is TRUE; priors on q for indices of
    ## abundance
    if (priors$q_IA$use) {
      q.sample.IA <- replicate(num.IA, priors$q_IA$rfn())
    } else {
      ## FIXME: -9999 is probably not a good sentinel value here; NA?
      q.sample.IA <- rep(-9999, length(unique(rel.abundance$Index)))
    }

    ##priors on q for count data
    if (priors$q_count$use) {
      q.sample.Count <- replicate(num.Count, priors$q_count$rfn())
    } else {
      ## FIXME: Sentinel -9999 again
      q.sample.Count <- rep(-9999, length(unique(count.data$Index)))
    }

    sample.K <- LOGISTIC.BISECTION.K(K.low = control$K_bisect_lim[1],
                                     K.high = control$K_bisect_lim[2],
                                     r_max = sample.r_max,
                                     z = sample.z,
                                     num_Yrs = bisection.Yrs,
                                     start_Yr = start_Yr,
                                     target.Pop = sample.N.obs,
                                     catches = catches,
                                     MVP = MVP,
                                     tol = control$K_bisect_tol)

    #Computing the predicted abundances with the samples from the priors
    #----------------------------------------
    Pred_N <- GENERALIZED_LOGISTIC(r_max = sample.r_max,
                                   K = sample.K,
                                   N1 = sample.K,
                                   z = sample.z,
                                   start_Yr = start_Yr,
                                   num_Yrs = projection.Yrs,
                                   catches = catches,
                                   MVP = MVP)


    #Computing the predicted ROI for the IAs and Count data, if applicable
    #----------------------------------------
    #For IAs
    if (rel.abundance.key) {
      Pred.ROI.IA <- COMPUTING.ROI(data = rel.abundance,
                                   Pred_N = Pred_N$Pred_N,
                                   start_Yr = start_Yr)
    } else {
      Pred.ROI.IA <- rep(0, num.IA)
    }

    #For Count Data
    if (count.data.key) {
      Pred.ROI.Count <- COMPUTING.ROI(data = count.data,
                                      Pred_N = Pred_N$Pred_N,
                                      start_Yr = start_Yr)
    } else {
      Pred.ROI.Count <- rep(0, num.Count)
    }

    #Calculate Analytical Qs if rel.abundance.key is TRUE
    #---------------------------------------------------------
    if (rel.abundance.key) {
      if (!priors$q_IA$use) {
        q.sample.IA <- CALC.ANALYTIC.Q(rel.abundance,
                                       Pred_N$Pred_N,
                                       start_Yr,
                                       sample.add_CV,
                                       num.IA)
      } else {
      q.sample.IA <- q.sample.IA
      }
    }

    #browser()

    ## Calculate Analytical Qs if count.data.key is TRUE
    ## (NOT USED YET - AZerbini, Feb 2013)
    if (rel.abundance.key) {
      if (!priors$q_count$use) {
        q.sample.Count <- CALC.ANALYTIC.Q(count.data,
                                          Pred_N$Pred_N,
                                          start_Yr,
                                          sample.add_CV,
                                          num.Count)
      } else {
      q.sample.Count <- q.sample.Count
      }
    }

    if (control$verbose > 3) {
      message("r_max = ", sample.r_max,
              " N.obs = ", sample.N.obs,
              " K = ", sample.K,
              " Pred_N.target = ", Pred_N$Pred_N[bisection.Yrs],
              " q.IAs = ", q.sample.IA,
              " q.Count = ", q.sample.Count)
    }

    #Compute the likelihoods
    #--------------------------------
    # (1) relative indices (if rel.abundance.key is TRUE)
    if (rel.abundance.key) {
      lnlike.IAs <- LNLIKE.IAs(rel.abundance,
                               Pred_N$Pred_N,
                               start_Yr,
                               q.sample.IA,
                               sample.add_CV,
                               TRUE)
    } else {
      lnlike.IAs <- 0
    }
    if (control$verbose > 1) {
    }

    # (2) count data (if count.data.key is TRUE)
    if (count.data.key) {
      lnlike.Count <- LNLIKE.IAs(count.data,
                                 Pred_N$Pred_N,
                                 start_Yr,
                                 q.sample.Count,
                                 sample.add_CV,
                                 log=TRUE)
    } else {
      lnlike.Count <- 0
    }
    if (control$verbose > 1) {
    }

    # (3) absolute abundance
    lnlike.Ns <- LNLIKE.Ns(abs.abundance,
                           Pred_N$Pred_N,
                           start_Yr,
                          sample.add_CV,
                           log=TRUE)

    # (4) growth rate if applicable
    if (growth.rate.obs[3]) {
      ## gr_idx <- 
      Pred.GR <- calc_growth_rate(years = growth.rate.Yrs[c(1, 4)],
                                  pred_pop = data.frame(Pred_N = Pred_N$Pred_N,
                                                    year = start_Yr:end.Yr))
      lnlike.GR <- LNLIKE.GR(Obs.GR=growth.rate.obs[1],
                             Pred.GR=Pred.GR,
                             GR.SD.Obs=growth.rate.obs[2])
    } else {
      lnlike.GR <- 0
    }

    if (control$verbose > 2) {
      message("lnlike.IAs = ", lnlike.IAs,
              " lnlike.Count = ", lnlike.Count,
              " lnlike.Ns = ", lnlike.Ns,
              " lnlike.GR = ", lnlike.GR)
    }

    ## These use the likelihoods in Zerbini et al. (2011)
    LL <- lnlike.IAs[[1]] + lnlike.Count[[1]] + lnlike.Ns[[1]] + lnlike.GR[[1]]
    Likelihood <- exp(-LL)
    if (control$verbose > 1) {
      message("NLL = ", LL,
              " Likelihood = ", Likelihood)
    }

    if (Pred_N$Violate_Min_Viable_Pop) {
      Likelihood <- 0
      if (control$verbose > 0) {
        message("MVP violated on draw", draw)
      }
    }

    Cumulative.Likelihood <- Cumulative.Likelihood + Likelihood

    if (!Pred_N$Violate_Min_Viable_Pop) {
      while (Cumulative.Likelihood > control$threshold) {
        if (control$verbose > 0) {
          message("sample = ", i, " draw = ", draw)
        }
        if (control$verbose > 1) {
          message("draw = ", draw,
                  " Likelihood = ", Likelihood,
                  " Cumulative = ", Cumulative.Likelihood)
        }
        save <- TRUE
        Cumulative.Likelihood <- Cumulative.Likelihood-control$threshold
        resamples.trajectories <- rbind(resamples.trajectories, Pred_N$Pred_N)
        resamples.output <- rbind(resamples.output,
                                  c(sample.r_max,
                                    sample.K,
                                    sample.N.obs,
                                    sample.add_CV,
                                    Pred_N$Min_Pop,
                                    Pred_N$Min_Yr,
                                    Pred_N$Violate_Min_Viable_Pop,
                                    c(Pred_N$Pred_N[output.Yrs - start_Yr + 1]),
                                    Pred.ROI.IA,
                                    q.sample.IA,
                                    Pred.ROI.Count,
                                    q.sample.Count,
                                    lnlike.IAs[[1]],
                                    lnlike.Count[[1]],
                                    lnlike.Ns[[1]],
                                    lnlike.GR[[1]],
                                    LL,
                                    Likelihood,
                                    Pred_N$Min_Pop / sample.K,
                                    c(Pred_N$Pred_N[output.Yrs - start_Yr + 1] /
                                      sample.K),
                                    draw,
                                    save))
        i <- i+1
        if (control$progress_bar) {
          setTxtProgressBar(pb, i)
        }
      }
    }

    samples.output <- rbind(samples.output,
                            c(sample.r_max,
                              sample.K,
                              sample.N.obs,
                              sample.add_CV,
                              Pred_N$Min_Pop,
                              Pred_N$Min_Yr,
                              Pred_N$Violate_Min_Viable_Pop,
                              c(Pred_N$Pred_N[output.Yrs-start_Yr+1]),
                              Pred.ROI.IA,
                              q.sample.IA,
                              Pred.ROI.Count,
                              q.sample.Count,
                              lnlike.IAs[[1]],
                              lnlike.Count[[1]],
                              lnlike.Ns[[1]],
                              lnlike.GR[[1]],
                              LL,
                              Likelihood,
                              Pred_N$Min_Pop/sample.K,
                              c(Pred_N$Pred_N[output.Yrs-start_Yr+1]/sample.K),
                              draw,
                              save))

    draw <- draw+1
  }

  samples.output <- data.frame(samples.output)
  names(samples.output) <- names
  samples.output <- samples.output[-1, ]
  samples.output.summary <- SUMMARY.SIR(x=samples.output, scenario = file.name)
  ## FIXME Use `paste0` here
  write.csv(samples.output,
            paste(file.name, "_", "samples.output.csv", sep=""))

  resamples.output <- data.frame(resamples.output)
  names(resamples.output) <- names
  resamples.output <- resamples.output[-1, ]
  resamples.output.summary <- SUMMARY.SIR(x = resamples.output,
                                          scenario = file.name)
  ## FIXME Use `paste0` here
  write.csv(resamples.output,
            paste(file.name, "_", "resamples.output.csv", sep=""))

  resamples.trajectories <- data.frame(resamples.trajectories)
  names(resamples.trajectories) <- seq(start_Yr, end.Yr,  1)
  resamples.trajectories <- resamples.trajectories[-1, ]
  ## FIXME Use `paste0` here
  write.csv(resamples.trajectories,
            paste(file.name, "_", "resample.trajectories.csv",  sep=""))

  ## FIXME Use one quantile call to get all?
  final.trajectory[, 1] <- sapply(resamples.trajectories, mean)
  final.trajectory[, 2] <- sapply(resamples.trajectories, median)
  final.trajectory[, 3] <- sapply(resamples.trajectories, quantile,
                                  probs = c(0.025))
  final.trajectory[, 4] <- sapply(resamples.trajectories, quantile,
                                  probs = c(0.975))
  final.trajectory[, 5] <- sapply(resamples.trajectories, quantile,
                                  probs = c(0.05))
  final.trajectory[, 6] <- sapply(resamples.trajectories, quantile,
                                  probs = c(0.95))
  final.trajectory <- data.frame(final.trajectory)
  names(final.trajectory) <- c("mean", "median",
                               "PI.2.5%", "PI.97.5%",
                               "PI.5%", "PI.95%")
  final.trajectory <- data.frame(Year, final.trajectory)

  resamples.per.samples <- dim(samples.output)[1] / dim(resamples.output)[1]

  end.time <- Sys.time()
  if (control$verbose > 0) {
    message("Time to Compute = ", (end.time-begin.time))
  }

  list(call = call,
       file.name = file.name,
       Date.Time = Sys.time(),
       Time.to.compute.in.minutes = paste((end.time-begin.time) / 60),
       threshold = control$threshold,
       Ratio.Resamples.per.Sample = paste("1 resample",
                                          ":",
                                          resamples.per.samples,
                                          "samples"),
       resamples.output = resamples.output,
       resamples.output.summary = resamples.output.summary$output.table,
       samples.output.summary = samples.output.summary$output.table,
       final.trajectory = final.trajectory,
       inputs = list(draws = draw,
                     n.resamples = n.resamples,
                     prior_r_max = priors$r_max,
                     priors_N.obs = priors$N.obs,
                     target.Yr = target.Yr,
                     MVP = paste("num.haplotypes = ",
                                 num.haplotypes,
                                 "MVP = ",
                                 4 * num.haplotypes),
                     tolerance = control$K_bisect_tol,
                     output.Years = output.Yrs))
}


#' Computes the predicted rate of increase for a set of specified years for
#' comparison with trends estimated separately with any of the indices of
#' abundance or count data
#'
#' @param data Count data or relative abundance index to use
#' @param Pred_N Number of individuals predicted
#' @param start_Yr Initial year
#'
#' @return Vector of rates of increase, one per index
#' @export
#'
#' @examples
COMPUTING.ROI <- function(data = data, Pred_N = Pred_N, start_Yr = NULL) {
  num.indices <- max(data$Index)
  Pred.ROI <- rep(NA, num.indices)

  for (i in 1:num.indices) {
    index.ini.year <- (head(subset(data, Index == i)$Year, 1) - start_Yr)
    index.final.year <- (tail(subset(data, Index == i)$Year, 1) - start_Yr)
    elapsed.years <- index.final.year - index.ini.year

    Pred.ROI[i] <- exp((log(Pred_N[index.final.year]) -
                        log(Pred_N[index.ini.year])) /
                       (elapsed.years)) - 1
  }
  Pred.ROI
}

#' Calculate a target K for the bisection method
#'
#' @param r_max The maximum net recruitment rate ($r_{max}$).
#' @param K Pre-expoitation population size in numbers or biomass
#'   (depending on input).
#' @param N1 Population size in numbers or biomass at year 1 (generally
#'   assumed to be K).
#' @param z Generalized logistic shape parameter, determines population
#'   size where productivity is masimum (assumed to be 2.39 by the ISC
#'   SC).
#' @param num_Yrs The number of projection years. Set as the last year
#'   in the catchor abundance series whichever is most recent, minus the
#'   start year.
#' @param start_Yr First year of the projection (assumed to be the first
#'   year in the catch series).
#' @param target.Pop Target population size.
#' @param catches Catch time series. Cannot include NAs,
#' @param MVP Minimum Viable Population Size; `4 * num.haplotypes`
#'
#' @return Vector of differences between predicted population and target
#'   population.
#' @export
#'
#' @examples
#' TARGET.K(r_max, K, N1, z, start_Yr=start_Yr, num_Yrs=bisection.Yrs,
#'          target.Pop=target.Pop, catches=catches, MVP=MVP)
TARGET.K <- function(r_max, K, N1, z,
                     num_Yrs, start_Yr,
                     target.Pop, catches,
                     MVP = 0) {

  Pred_N <- GENERALIZED_LOGISTIC(r_max = r_max,
                                 K = K,
                                 N1 = K,
                                 z = z,
                                 start_Yr = start_Yr,
                                 num_Yrs = num_Yrs,
                                 catches = catches,
                                 MVP = MVP)
  Pred_N$Pred_N[num_Yrs] - target.Pop
}

#' LOGISTIC BISECTION
#'
#' Method of Butterworth and Punt (1995) where the prior distribution of the
#' current absolute abundance $N_{2005}$ and maximum net recruitment rate
#' \code{r_max} are sampled and then used to determine the unique value of the
#' population abundance $N$ in \code{start_Yr} (assumed to correspond to
#' carrying capacity $K$). Requires \code{\link{TARGET.K}} and subsequent
#' dependencies.
#'
#' @param K.low Lower bound for $K$ when preforming the bisection method of Punt
#'   and Butterworth (1995). Default is 1.
#' @param K.high Upper bound for $K$ when preforming the bisection method of
#'   Punt and Butterworth (1995). Default is 500,000.
#' @param r_max The maximum net recruitment rate ($r_{max}$).
#' @param z The parameter that determines the population size where productivity
#'   is maximum (assumed to be 2.39 by the IWC SC).
#' @param num_Yrs The number of projection years. Set as the last year in the
#'   catch or abundance series, whichever is most recent, minus the
#'   \code{start_Yr}.
#' @param start_Yr The first year of the projection (assumed to be the first
#'   year in the catch series).
#' @param target.Pop A sample of the prior on population abundance $N$, in
#'   numbers, set as \code{sample.N.obs} sampled from \code{priors$N.obs}
#' @param catches The time series of catch in numbers or biomass. Currently does
#'   not handle NAs and zeros will have to input a priori for years in which
#'   there were no catches.
#' @param MVP The minimum viable population size in numbers or biomass. Computed
#'   as 4 * \code{\link{num.haplotypes}} to compute minimum viable population
#'   (from Jackson et al., 2006 and IWC, 2007).
#' @param tol The desired accuracy (convergence tolerance) of
#'   \code{\link{stats::uniroot}}.
#'
#' @return A numeric scalar of an estimate of  carrying capacity $K$.
#'
#' @examples
#' LOGISTIC.BISECTION.K(K.low = 1, K.high = 100000, r_max = r_max, z = z,
#'                      num_Yrs = bisection.Yrs, start_Yr = start_Yr,
#'                      target.Pop = target.Pop, catches = catches, MVP = MVP,
#'                      tol = 0.001)
LOGISTIC.BISECTION.K <- function(K.low,
                                 K.high,
                                 r_max,
                                 z,
                                 num_Yrs,
                                 start_Yr,
                                 target.Pop,
                                 catches,
                                 MVP,
                                 tol = 0.001) {
  Kmin <- uniroot(TARGET.K,
                  tol = tol,
                  c(K.low,
                    K.high),
                  r_max = r_max,
                  z = z,
                  num_Yrs = num_Yrs,
                  start_Yr = start_Yr,
                  target.Pop = target.Pop,
                  catches = catches,
                  MVP = MVP)
  Kmin$root
}

#' Compute analytic estimates of q, the scaling parameter between indices and
#' absolute population size
#'
#' @param rel.Abundance Relative abundance index
#' @param add_CV Coefficient of variation
#' @param Pred_N Predicted population
#' @param start_Yr Initial year
#' @param num.IA Index of abundance
#'
#' @return A numeric estimator for $q$.
#' @export
#'
#' @examples
CALC.ANALYTIC.Q <- function(rel.Abundance, Pred_N, start_Yr,
                            add_CV = 0, num.IA) {
  ## Vector to store the q values
  analytic.Q <- rep(NA, num.IA)

  for (i in 1:num.IA) {
    ## Subseting across each index of abundance
    IA <- Rel.Abundance[Rel.Abundance$Index == i,]
    ## Years for which IAs are available
    IA.yrs <- IA$Year-start_Yr + 1
    ## Computing the value of sigma as in Zerbini et al. 2011
    IA$Sigma <- sqrt(log(1 + IA$CV.IA.obs^2))
    ## Numerator of the analytic q estimator (Zerbini et al., 2011 - eq. (3))
    qNumerator <- sum((log(IA$IA.obs / Pred_N[IA.yrs])) /
                      (IA$Sigma * IA$Sigma + add_CV * add_CV))
    ## Denominator of the analytic q estimator (Zerbini et al., 2011 - eq. (3))
    qDenominator <- sum(1 / (IA$Sigma * IA$Sigma))
    ## Estimate of q
    analytic.Q[i] <- exp(qNumerator / qDenominator)
  }
  analytic.Q
}

#' Compute the log-likelihood of indices of abundance
#'
#' @param Rel.Abundance Relative abundance
#' @param Pred_N Predicted population size
#' @param start_Yr Initial year
#' @param q.values Scaling parameter
#' @param add.CV Coefficient of variation
#' @param log Boolean, return log likelihood (default TRUE) or
#'   likelihood.
#'
#' @return List of likelihood based on Zerbini et al. (2011) eq. 5 or using `dnorm`
#' @export
#'
#' @examples
LNLIKE.IAs <- function(Rel.Abundance, Pred_N, start_Yr,
                       q.values, add.CV, log = TRUE) {
    loglike.IA1 <- 0
    IA.yrs <- Rel.Abundance$Year-start_Yr + 1
    loglike.IA1 <- -sum(
        dlnorm_zerb( # NOTE: can be changed to dlnorm
        x = Rel.Abundance$IA.obs,
        meanlog = log( q.values[Rel.Abundance$Index] * Pred_N[IA.yrs] ),
        sdlog = Rel.Abundance$Sigma + add.CV,
        log))

    loglike.IA1
}


#' LOG LIKELIHOOD OF ABSOLUTE ABUNDANCE
#'
#' This function computes two estimates of the log-likelihood of the estimated
#' absolute abundance using the equation from Zerbini et al. 2011 (eq. 4) and a
#' lognormal distribution from \code{\link{CALC.LNLIKE}}.
#'
#' @param Obs.N Observed absoluted abundance in numbers as a data.frame
#'   containing year, estimate of absolute abundance, and CV.
#' @param Pred_N Predicted absolute abundance in numbers from
#'   \code{\link{GENERALIZED_LOGISTIC}}.
#' @param start_Yr The first year of the projection (assumed to be the first
#'   year in the catch series).
#' @param add_CV Additional CV to add to variance of lognormal distribution
#'   sampled from \code{priors$add_CV}.
#' @param log Return the log of the likelihood (TRUE/FALSE)
#'
#' @return A list of two numeric scalars of estimates of log-likelihood.
#'
#' @examples
#' Obs.N  <-  data.frame(Year = 2005, Sigma = 5, Obs.N = 1000)
#' Pred_N  <-  1234
#' start_Yr  <-  2005
#' LNLIKE.Ns(Obs.N, Pred_N, start_Yr, add_CV = 0, log=TRUE)
LNLIKE.Ns <- function(Obs.N, Pred_N, start_Yr, add_CV, log = TRUE) {
  loglike.Ns1 <- 0
  loglike.Ns2 <- 0

  ## Years for which Ns are available
  N.yrs <- Obs.N$Year-start_Yr+1
  ## This is the likelihood from Zerbini et al. 2011 (eq. 4)
  loglike.Ns1 <- loglike.Ns1 +
    ((sum(log(Obs.N$Sigma) + log(Obs.N$N.obs) + 0.5 *
          ((((log(Pred_N[N.yrs]) - log(Obs.N$N.obs))^2) /
            (Obs.N$Sigma * Obs.N$Sigma + add_CV * add_CV))))))
  ## This is the log-normal distribution from R (using function dnorm)
  ## FIXME See comments above re: `dlnorm`
  loglike.Ns2 <- loglike.Ns2 + CALC.LNLIKE(Obs.N = Obs.N$N.obs,
                                           Pred_N = (Pred_N[N.yrs]),
                                           CV = sqrt(Obs.N$Sigma * Obs.N$Sigma +
                                                     add_CV * add_CV),
                                           log = log)

  list(loglike.Ns1 = loglike.Ns1, loglike.Ns2 = loglike.Ns2)
}

#' Calculate the log-likelihood of the growth rate
#'
#' Calculates the log-likelihood of the estimated growth rate given the observed
#' growth rate and the standard deviation of the observed growth rate.
#'
#' @param Obs.GR Observed growth rate
#' @param Pred.GR Predicted growth rate
#' @param GR.SD.Obs Standard error of the observed growth rate
#'
#' @return A \code{list} containing \code{loglike.GR1} and \code{loglike.GR2}
#'
#' @examples
#' LNLIKE.GR(0.1, 0.1, 0.1)
LNLIKE.GR <- function(Obs.GR, Pred.GR, GR.SD.Obs) {
  ## TODO Does this need to recalculate and return *both* of these values?
  loglike.GR1 <- 0
  loglike.GR2 <- 0

  ## This is the likelihood from Zerbini et al. 2011 (eq. 6)
  loglike.GR1 <- loglike.GR1 + (((log(GR.SD.Obs) + 0.5 * (((Pred.GR-Obs.GR) / GR.SD.Obs)^2))))

  ## loglike.GR2 <- loglike.GR2 + CALC.LNLIKE(Obs.N = Obs.GR,
  ##                                          Pred_N = Pred.GR,
  ##                                          CV = GR.SD.Obs,
  ##                                          log = FALSE)

  list(loglike.GR1 = loglike.GR1, loglike.GR2 = loglike.GR2)
}

#' Function to calculate the log-likelihood using a lognormal distribution
#'
#' @param Obs.N Time series of observed abundance
#' @param Pred_N Time series of estimated abundance
#' @param CV coefficient of variation
#' @param log whether to export as log-likelihood
#'
#' @return returns a scalar of the likelihood
#'
#' @examples
#' Obs.N <- 2000
#' Pred_N <- 2340
#' CV <- 4
#' CALC.LNLIKE(Obs.N, Pred_N, CV)
CALC.LNLIKE <- function(Obs.N, Pred_N, CV, log = FALSE) {
  sum(dnorm(x = log(Obs.N), mean = log(Pred_N), sd = CV, log = log))
}

#' OUTPUT FUNCTION
#'
#' Function that provides a summary of SIR outputs including: mean, median, 95%
#' credible interval, 90% predicitive interval, max, and sample size.
#'
#' @param x A data.frame of model outputs including: sample.r_max, sample.K,
#'   sample.N.obs, sample.add_CV, Pred_N$Min_Pop, Pred_N$Min_Yr,
#'   Pred_N$Violate_Min_Viable_Pop, c(Pred_N$Pred_N[output.Yrs-start_Yr+1]), Pred.ROI.IA,
#'   q.sample.IA, Pred.ROI.Count, q.sample.Count, lnlike.IAs[[1]],
#'   lnlike.Count[[1]], lnlike.Ns[[1]], lnlike.GR[[1]], LL, Likelihood,
#'   Pred_N$Min_Pop/sample.K, c(Pred_N$Pred_N[output.Yrs-start_Yr+1]/sample.K),
#'   draw, save)
#' @param scenario Name of the model run and object as specified by the user.
#'
#' @return Returns a data.frame with summary of SIR outputs
#'
#' @examples
#' x  <-  rnorm(1000, 5, 7)
#' y  <-  rnorm(1000, 6, 9)
#' df <- data.frame(x = x, y = y)
#' SUMMARY.SIR( df , scenario = "example_summary")
SUMMARY.SIR <- function(x, scenario = "USERDEFINED") {
  num.col <- dim(x)[2]
  col.names <- names(x)
  row.names <- c("mean", "median",
                 "2.5%PI", "97.5%PI",
                 "5%PI", "95%PI",
                 "min", "max", "n")

  ## FIXME Only call quantile once?
  output.summary <- matrix(nrow = length(row.names), ncol = num.col)
  output.summary[1, ] <- sapply(x, mean)
  output.summary[2, ] <- sapply(x, median)
  output.summary[3, ] <- sapply(x, quantile, probs=0.025)
  output.summary[4, ] <- sapply(x, quantile, probs=0.975)
  output.summary[5, ] <- sapply(x, quantile, probs=0.05)
  output.summary[6, ] <- sapply(x, quantile, probs=0.95)
  output.summary[7, ] <- sapply(x, min)
  output.summary[8, ] <- sapply(x, max)
  output.summary[9, ] <- sapply(x, length)

  output.table <- data.frame(output.summary)
  names(output.table) <- col.names
  row.names(output.table) <- row.names
  noquote(format(output.table, digits = 3, scientific = FALSE))

  list(scenario=scenario, date=Sys.time(), output.table=output.table)
}

